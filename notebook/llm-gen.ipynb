{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87806775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ljw/Documents'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d98ba58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 6076\r\n",
      "-rw------- 1 root 5218936 Jun 28 11:59  IPCC_AR6_SYR_LongerReport.pdf\r\n",
      "-rw-r--r-- 1 root   38223 Jul  1 10:08 'llm_gen_TODO_local invoke Qwen using Transformer and ollama.ipynb'\r\n",
      "-rw-r--r-- 1 root    2544 Jul  1 11:46  llm.py\r\n",
      "-rw-r--r-- 1 root  179093 Jun 27 12:42  lm-eval.ipynb\r\n",
      "-rw-r--r-- 1 root  350225 Jun 28 11:54  opencompass.ipynb\r\n",
      "-rw-r--r-- 1 root  419337 Jun 28 16:40  ragas.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3438840",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting ipywidgets\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/d4/17/8b2ce5765dd423433d2e0727712629c46152fb0bc706b0977f847480f262/ipywidgets-8.1.3-py3-none-any.whl (139 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.11 (from ipywidgets)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/93/c1/68423f43bc95d873d745bef8030ecf47cd67f932f20b3f7080a02cff43ca/widgetsnbextension-4.0.11-py3-none-any.whl (2.3 MB)\n",
      "Collecting jupyterlab-widgets~=3.0.11 (from ipywidgets)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/65/f6/659ca44182c86f57977e946047c339c717745fda9f43b7ac47f274e86553/jupyterlab_widgets-3.0.11-py3-none-any.whl (214 kB)\n",
      "Requirement already satisfied: decorator in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.11.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.3 jupyterlab-widgets-3.0.11 widgetsnbextension-4.0.11\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f6c1b",
   "metadata": {},
   "source": [
    "### LLM Invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db144be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting langchain_huggingface\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/39/ce/ad7f50a6289cf562747df9a966b4d60a848571db2e57ad8d00dca2478096/langchain_huggingface-0.0.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from langchain_huggingface) (0.23.4)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from langchain_huggingface) (0.2.11)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/58/4b/922436953394e1bfda05e4bf1fe0e80f609770f256c59a9df7a9254f3e0d/sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/40/4f/eb78de4af3b17b589f43a369cbf0c3a7173f25c3d2cd93068852c07689aa/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Collecting transformers>=4.39.0 (from langchain_huggingface)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/20/5c/244db59e074e80248fdfa60495eeee257e4d97c3df3487df68be30cd60c8/transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.11.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.1.84)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.5.0)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/cb/e2/1bd899d3eb60c6495cf5d0d2885edacac08bde7a1407eadeb2ab36eca3c7/torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "Requirement already satisfied: numpy in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f2/60/6c589c91e474721efdcec82ea9cc5c743359e52637e46c364ee5236666ef/scikit_learn-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/e2/20/15c8fe0dfebb6facd81b3d08bf45dfa080e305deb17172b0a40eba59e927/scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.1 MB)\n",
      "Collecting Pillow (from sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b5/5b/6651c288b08df3b8c1e2f8c1152201e0b25d240e22ddade0f1e242fc9fa0/pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.5.15)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.39.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/8f/05/969e1a976b84283285181b00028cf73d78434b77a6627fc2a94194cca265/safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/llm-gen/lib/python3.10/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.7.4)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/61/53/e18c8c97d0b2724d85c9830477e3ebea3acf1dcdc6deb344d5d9c93a9946/sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/38/e9/5f72929373e1a0e8d142a130f3f97e6ff920070f87f91c4e13e40e0fba5a/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/31/80/3a54838c3fb461f6fec263ebf3a3a41771bd05190238de3486aae8540c36/jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/4b/2a/0a131f572aa09f741c30ccd45a8e56316e8be8dfc7bc19bf0ab7cfef7b19/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/da/d3/8057f0587683ed2fcd4dbfbdfdfa807b9160b809976099d36b8f60d08f03/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==2.3.1 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/d7/69/8a9fde07d2d27a90e16488cdfe9878e985a247b2496a4b5b1a2126042528/triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/75/bc/e0d0dbb85246a086ab14839979039647bce501d8c661a159b8b019d987b7/nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/7c/52/2b1b570f6b8b803cef5ac28fdf78c0da318916c7d2fe9402a84d591b394c/MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface)\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, triton, threadpoolctl, sympy, scipy, safetensors, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, joblib, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence-transformers, langchain_huggingface\n",
      "Successfully installed MarkupSafe-2.1.5 Pillow-10.4.0 jinja2-3.1.4 joblib-1.4.2 langchain_huggingface-0.0.3 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.3 scikit-learn-1.5.1 scipy-1.14.0 sentence-transformers-3.0.1 sympy-1.12.1 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.3.1 transformers-4.42.3 triton-2.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b986158",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a5b0cb5efc4bdc8dc476f00362a4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: hello\n",
      "\n",
      "Answer: Let's think step by step. The question is asking for a greeting. A common greeting is \"hello\". So the answer is hello.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "model_id = \"/opt/loc-llm/Qwen1.5-4B\"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    device=0,  # replace with device_map=\"auto\" to use the accelerate library.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
    ")\n",
    "\n",
    "gpu_chain = prompt | gpu_llm.bind(skip_prompt=True)\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(gpu_chain.invoke(\"hello\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c5ca8",
   "metadata": {},
   "source": [
    "### Transformer -> OutOfMemoryError: CUDA out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e49b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15306c4ef36a4cc1af60d717ce8e193e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m\n\u001b[1;32m      8\u001b[0m template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m请使用以下内容作为基础生成中文的Excel问答对，并确保做到以下要求：\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\t- Excel表格形式返回：将生成的问答对以标准的 Excel 表格呈现。\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     23\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template)\n\u001b[0;32m---> 24\u001b[0m gpu_llm \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFacePipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model_id\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# replace with device_map=\"auto\" to use the accelerate library.\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_new_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(gpu_llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m请生成一条中文的问答对，问题随意，但用excel格式表示\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     34\u001b[0m gpu_chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m gpu_llm\u001b[38;5;241m.\u001b[39mbind(skip_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/langchain_huggingface/llms/huggingface_pipeline.py:217\u001b[0m, in \u001b[0;36mHuggingFacePipeline.from_model_id\u001b[0;34m(cls, model_id, task, backend, device, device_map, model_kwargs, pipeline_kwargs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     _model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    214\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _model_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     }\n\u001b[1;32m    216\u001b[0m _pipeline_kwargs \u001b[38;5;241m=\u001b[39m pipeline_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 217\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mhf_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_pipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m VALID_TASKS:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot invalid task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrently only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mVALID_TASKS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1097\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1095\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m-> 1097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:96\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     98\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/transformers/pipelines/base.py:894\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate as well as the case that model is already on device\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    893\u001b[0m ):\n\u001b[0;32m--> 894\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;66;03m# Update config and generation_config with task specific parameters\u001b[39;00m\n\u001b[1;32m    897\u001b[0m task_specific_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_specific_params\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/transformers/modeling_utils.py:2796\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2792\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2795\u001b[0m         )\n\u001b[0;32m-> 2796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm-gen/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# %load llm.py\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "model_id = \"/opt/loc-llm/Qwen1.5-7B-Chat/\"\n",
    "\n",
    "template = \"\"\"\n",
    "请使用以下内容作为基础生成中文的Excel问答对，并确保做到以下要求：\n",
    "\t- Excel表格形式返回：将生成的问答对以标准的 Excel 表格呈现。\n",
    "\n",
    "Excel表格要求:\n",
    "1. 确保包含至少以下列:\n",
    "\t- 问题编号（QID）: 用于标识每个问题的独特性；\n",
    "    - 问题内容（Question）: 每个具体的问题表述；\n",
    "    - 答案描述（Answer）: 对应于每个问题的详细且准确解答。\n",
    "    \n",
    "{context}\n",
    "请确保您的回答遵循这些指示，并在最终结果中体现。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    device=0,  # replace with device_map=\"auto\" to use the accelerate library.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 20},\n",
    ")\n",
    "\n",
    "\n",
    "print(gpu_llm.invoke(\"请生成一条中文的问答对，问题随意，但用excel格式表示\"))\n",
    "\n",
    "gpu_chain = prompt | gpu_llm.bind(skip_prompt=True)\n",
    "\n",
    "context = \"\"\"\n",
    "在人工智能领域，由于大模型（LLM）技术的发展以及其广阔的市场前景，MaaS 以及开源大模型呈现出百家争鸣的景象。现阶段，大型语言模型的开发和应用已经成为各个领域智能化提升的重要方向。为了利用大模型实现业务和产品的提升或创新，就需要对大模型进行系统的评估。\n",
    "LLM评估是指在人工智能系统中评估和改进语言和语言模型的过程。在人工智能领域，特别是在自然语言处理（NLP）及相关领域，LLM评估具有至高无上的地位。通过评估语言生成和理解模型，LLM评估有助于细化人工智能驱动的语言相关任务和应用程序，确保在语言发挥关键作用的各种场景中增强准确性和适应性。\n",
    "随着大模型的版本升级和应用的持续，对大模型的评估也绝非一次性，而是需要多次迭代的过程。建立一个有效的、可持续的评估过程非常重要。如今，许多大模型服务通过LLMOps实现了CI、CE、CD（持续集成、持续评估、持续部署），大大提高了大模型的可用性。但在大模型微调、开源大模型应用、基于RAG的架构以及行业应用等层面，我们仍然需要了解大模型的评测技术。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(gpu_chain.invoke({\"context\": context}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e683b4",
   "metadata": {},
   "source": [
    "### ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b373772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          \tID          \tSIZE  \tMODIFIED    \r\n",
      "mistral:latest\t2ae6f6dd7a3d\t4.1 GB\t10 days ago\t\r\n",
      "llama2:latest \t78e26419b446\t3.8 GB\t10 days ago\t\r\n",
      "phi3:3.8b     \t64c1188f2485\t2.4 GB\t13 days ago\t\r\n",
      "phi3:14b      \t1e67dff39209\t7.9 GB\t13 days ago\t\r\n",
      "phi3:latest   \t64c1188f2485\t2.4 GB\t13 days ago\t\r\n",
      "llama3:8b     \t365c0bd3c000\t4.7 GB\t13 days ago\t\r\n",
      "qwen:72b      \te1c64582de5c\t41 GB \t13 days ago\t\r\n",
      "qwen2:latest  \te0d4e1163c58\t4.4 GB\t13 days ago\t\r\n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95772ce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "\"问题编号\",\"问题内容\",\"答案描述\"\n",
      "\"Q1\",\"LLM评估是什么？\",\"LLM评估是在人工智能系统中评估和改进语言和语言模型的过程，对自然语言处理（NLP）及相关领域至关重要。它帮助细化AI驱动的语言相关任务和应用，确保各种场景中的准确性与适应性。\"\n",
      "\"Q2\",\"大模型的评估为何需要多次迭代？\",\"随着大模型的版本升级和应用持续发展，评估过程需多次迭代以确保其性能优化并保持竞争力。建立有效的评估流程对于提升大模型可用性至关重要。\"\n",
      "\"Q3\", \"LLMOps是什么？\",\"LLMOps是利用持续集成（CI）、持续评估（CE）和持续部署（CD）来管理大型语言模型（LLM）的一种实践方法，旨在提高大模型的服务质量和效率。\"\n",
      "\"Q4\", \"哪些方面需要了解关于大模型的评测技术？\",\"在大模型微调、开源大模型应用、基于RAG的架构以及行业应用层面，都需要了解相应的评测技术以优化模型性能和实际应用场景匹配度。\"\n",
      "```\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>```</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>问题编号</th>\n",
       "      <th>问题内容</th>\n",
       "      <td>答案描述</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1</th>\n",
       "      <th>LLM评估是什么？</th>\n",
       "      <td>LLM评估是在人工智能系统中评估和改进语言和语言模型的过程，对自然语言处理（NLP）及相关领...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q2</th>\n",
       "      <th>大模型的评估为何需要多次迭代？</th>\n",
       "      <td>随着大模型的版本升级和应用持续发展，评估过程需多次迭代以确保其性能优化并保持竞争力。建立有效...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q3</th>\n",
       "      <th>\"LLMOps是什么？\"</th>\n",
       "      <td>LLMOps是利用持续集成（CI）、持续评估（CE）和持续部署（CD）来管理大型语言模型（L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q4</th>\n",
       "      <th>\"哪些方面需要了解关于大模型的评测技术？\"</th>\n",
       "      <td>在大模型微调、开源大模型应用、基于RAG的架构以及行业应用层面，都需要了解相应的评测技术以优...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>```</th>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                           ```\n",
       "问题编号 问题内容                                                                 答案描述\n",
       "Q1   LLM评估是什么？               LLM评估是在人工智能系统中评估和改进语言和语言模型的过程，对自然语言处理（NLP）及相关领...\n",
       "Q2   大模型的评估为何需要多次迭代？         随着大模型的版本升级和应用持续发展，评估过程需多次迭代以确保其性能优化并保持竞争力。建立有效...\n",
       "Q3    \"LLMOps是什么？\"           LLMOps是利用持续集成（CI）、持续评估（CE）和持续部署（CD）来管理大型语言模型（L...\n",
       "Q4    \"哪些方面需要了解关于大模型的评测技术？\"  在大模型微调、开源大模型应用、基于RAG的架构以及行业应用层面，都需要了解相应的评测技术以优...\n",
       "```  NaN                                                                   NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2:latest\")\n",
    "\n",
    "template = \"\"\"\n",
    "请使用以下内容作为基础生成中文的CSV问答对，并确保做到以下要求：\n",
    "\t- CSV形式返回：将生成的问答对以标准的CSV呈现。\n",
    "\n",
    "CSV要求:\n",
    "1. 确保包含至少以下列:\n",
    "\t- 问题编号（QID）: 用于标识每个问题的独特性；\n",
    "    - 问题内容（Question）: 每个具体的问题表述；\n",
    "    - 答案描述（Answer）: 对应于每个问题的详细且准确解答。\n",
    "\n",
    "CSV格式举例说明：\n",
    "\t```\n",
    "\t\"问题编号\",\"问题内容\",\"答案描述\"\n",
    "\t\"Q1\",\"什么是LLM\",\"LLM是指大模型，也是AI\"\n",
    "\t```\n",
    "    \n",
    "{context}\n",
    "请确保您的回答遵循这些指示，并在最终结果中体现。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "context = \"\"\"\n",
    "在人工智能领域，由于大模型（LLM）技术的发展以及其广阔的市场前景，MaaS 以及开源大模型呈现出百家争鸣的景象。现阶段，大型语言模型的开发和应用已经成为各个领域智能化提升的重要方向。为了利用大模型实现业务和产品的提升或创新，就需要对大模型进行系统的评估。\n",
    "LLM评估是指在人工智能系统中评估和改进语言和语言模型的过程。在人工智能领域，特别是在自然语言处理（NLP）及相关领域，LLM评估具有至高无上的地位。通过评估语言生成和理解模型，LLM评估有助于细化人工智能驱动的语言相关任务和应用程序，确保在语言发挥关键作用的各种场景中增强准确性和适应性。\n",
    "随着大模型的版本升级和应用的持续，对大模型的评估也绝非一次性，而是需要多次迭代的过程。建立一个有效的、可持续的评估过程非常重要。如今，许多大模型服务通过LLMOps实现了CI、CE、CD（持续集成、持续评估、持续部署），大大提高了大模型的可用性。但在大模型微调、开源大模型应用、基于RAG的架构以及行业应用等层面，我们仍然需要了解大模型的评测技术。\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "csv_str = chain.invoke({\"context\": context})\n",
    "print(csv_str)\n",
    "from io import StringIO\n",
    "df = pd.read_csv(StringIO(csv_str))\n",
    "\n",
    "# 查看DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9731310",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting PyPDF2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8e/5e/c86a5643653825d3c913719e788e41386bee415c2b87b4f955432f2de6b2/pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa4cd9a",
   "metadata": {},
   "source": [
    "> 多次执行其结果以及基于context生成的问题未必正确，还是需要从中筛选，也从中可以看出要根据语义数据才能生成有效的问答对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f06dceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"问题编号\",\"问题内容\",\"问题来源\",\"答案描述\"\n",
      "\"Q1\",\"气候变化如何影响人类和自然系统？\",\"第2节, IPCC AR6 WGI\",\"气候变化已经影响了全球范围内的各种人类和自然系统。在受影响最大的地区，尽管人们贡献较小的气候变暖，但这些区域也最容易受到影响。\"\n",
      "\"Q2\",\"世界上不同地区的变化类型与人类贡献的认识有何关系？\",\"第1节, 第2节, IPCC AR6 WGI参考区域: 北美(NWN、NEN等), 中美洲(NCA), 南美洲(SAM、WAF等), 欧洲(GIC、NEU等), 非洲(MED、SAH等), 亚洲(RAR、ARP等)\",\"世界不同地区的变化类型与人类贡献的认识密切相关。高、中和低数据/文献支持表明了气候变化程度及人类活动影响。\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>问题编号</th>\n",
       "      <th>问题内容</th>\n",
       "      <th>问题来源</th>\n",
       "      <th>答案描述</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>气候变化如何影响人类和自然系统？</td>\n",
       "      <td>第2节, IPCC AR6 WGI</td>\n",
       "      <td>气候变化已经影响了全球范围内的各种人类和自然系统。在受影响最大的地区，尽管人们贡献较小的气候...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>世界上不同地区的变化类型与人类贡献的认识有何关系？</td>\n",
       "      <td>第1节, 第2节, IPCC AR6 WGI参考区域: 北美(NWN、NEN等), 中美洲(...</td>\n",
       "      <td>世界不同地区的变化类型与人类贡献的认识密切相关。高、中和低数据/文献支持表明了气候变化程度及...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  问题编号                       问题内容  \\\n",
       "0   Q1           气候变化如何影响人类和自然系统？   \n",
       "1   Q2  世界上不同地区的变化类型与人类贡献的认识有何关系？   \n",
       "\n",
       "                                                问题来源  \\\n",
       "0                                  第2节, IPCC AR6 WGI   \n",
       "1  第1节, 第2节, IPCC AR6 WGI参考区域: 北美(NWN、NEN等), 中美洲(...   \n",
       "\n",
       "                                                答案描述  \n",
       "0  气候变化已经影响了全球范围内的各种人类和自然系统。在受影响最大的地区，尽管人们贡献较小的气候...  \n",
       "1  世界不同地区的变化类型与人类贡献的认识密切相关。高、中和低数据/文献支持表明了气候变化程度及...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 读取PDF并提取文本\n",
    "file_path = './IPCC_AR6_SYR_LongerReport.pdf'\n",
    "llm = ChatOllama(model=\"qwen2:latest\")\n",
    "template = \"\"\"\n",
    "请使用以下内容作为基础生成中文的CSV问答对，并确保做到以下要求：\n",
    "\t- CSV格式返回：将生成的一道问答对以标准的CSV格式直接呈现。\n",
    "\n",
    "## CSV格式要求:\n",
    "1. 确保包含至少以下列:\n",
    "\t- 问题编号（QID）: 用于标识每个问题的独特性；\n",
    "    - 问题内容（Question）: 每个具体的问题表述；\n",
    "    - 问题来源（Context）: 抽取你觉得有用的可以作为参考生成问答对的PDF的`context的内容`\n",
    "    - 答案描述（Answer）: 对应于每个问题的详细且准确解答。\n",
    "2. 以**逗号**相隔\n",
    "\n",
    "## CSV格式举例说明：\n",
    "\t```\n",
    "\t\"问题编号\",\"问题内容\",\"问题来源\",\"答案描述\"\n",
    "\t\"Q1\",\"哪些因素影响了全球平均表面气温的变化\",\"The colour coding indicates the assessed conﬁdence in / likelihood76 of the observed change and the human contribution as a driver or main driver (speciﬁed in that case) \",\"全球平均表面气温变化受多种因素影响，包括但不限于气候变化系统组件的观测变化及对人类影响的归因。\"\n",
    "\t\"Q2\",\"在世界不同地区观察到的变化类型与对人类贡献的认识有什么关系？\",\"Section 2, IPCC AR6 WGI参考区域: 北美(NWN、NEN等), 中美洲(NCA), 南美洲(SAM、WAF等), 欧洲(GIC、NEU等), 非洲(MED、SAH等), 亚洲(RAR、ARP等)\",\"在世界不同地区观察到的变化类型与对人类贡献的认识关系密切，高、中和低程度的数据/文献支持表明了气候变化的程度及人类活动的影响。\"\n",
    "\t```\n",
    "\n",
    "## context的内容：\n",
    "```    \n",
    "{context}\n",
    "```\n",
    "\n",
    "请确保您的回答遵循这些指示，并在最终结果中体现。**请确保返回符合以逗号相隔的CSV格式的多条问答对，不需要其他多余的内容！**\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "pdf_file_obj = open(file_path, 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "\n",
    "page_num = 13\n",
    "page_obj = pdf_reader.pages[page_num]\n",
    "context = page_obj.extract_text()\n",
    "\n",
    "csv_str = chain.invoke({\"context\": context})\n",
    "from io import StringIO\n",
    "df = pd.read_csv(StringIO(csv_str))\n",
    "print(csv_str)\n",
    "print()\n",
    "df \n",
    "        \n",
    "# for page_num in range(len(pdf_reader.pages)):\n",
    "#     if page_num > 13:\n",
    "#         pdf_file_obj.close()\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llm-gen]",
   "language": "python",
   "name": "conda-env-llm-gen-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
